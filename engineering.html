<!DOCTYPE HTML>

<html>
	<head>
		<title>BO859545</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="stylesheet" href="assets/css/sec.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>

		<style>
		

		</style>
		
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<a href="index.html" class="logo"><strong>bo</strong> <span>859545</span></a>
						<nav>
							<a href="#menu">Menu</a>
						</nav>
					</header>

				<!-- Menu -->
					<nav id="menu">
						<ul class="links">
							<li><a href="index.html">Home</a></li>
							<li><a href="engineering.html">Engineering</a></li>
							<li><a href="reporting.html">Reporting</a></li>
							<li><a href="python.html">Programming</a></li>
							<li><a href="visualization.html">Visualization</a></li>
							<li><a href="ml.html">Machine Learning</a></li>
							<li><a href="big-data.html">Big Data</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main" class="alt">

						<!-- One -->
							<section id="one" style="background-color: white;">
								<div class="inner">
									<header class="major" style="margin: 0 auto;">
										 
										<!-- <nav id="sub-menu">
											<ul class="tertiary-nav" style="margin: auto 0;">
												<li><a href="engineering.html">Airflow</a></li>
												<li><a href="dagster.html">Dagster</a></li>
												<li><a href="template.html">AWS Glue</a></li>
												<li><a href="template.html">Azure Data Factory</a></li>
											</ul>
										</nav> -->
	
										<h2 style="color: #949494; text-align:center; margin-bottom: 0;" id="content">Building Data Pipeline</h2>
										<h4 style="color: #949494; text-align:center; margin-bottom: 0;" id="content">Data Acquisition</h4>
										
									</header>

									<p style="color: #545454; width: 70%; margin: 50px auto; text-align: justify; font-size: 19px;">This project demonstrates the implementation of a modern data stack and the development of a robust data pipeline for a retail company. It leverages a combination of Python for writing elt/etl scripts, <span class="highlight">Apache Airflow</span> for orchestration, <span class="highlight">BigQuery</span>, <span class="highlight">Snowflake</span>, <span class="highlight">AWS Redshift</span> for storage, <span class="highlight">dbt</span> for transformation and <span class="highlight">Amazon Managed Workflow for Apache Airiflow</span>  for production to efficiently move data from PostgreSQL database to BigQuery data warehouse for analytical processing.
										<!-- This end-to-end solution demonstrates best practices in data engineering and modern data management. -->
									</p>

									<!-- Content -->

									<h4 style="color: #949494; text-align:center; margin-bottom: 0;" id="content">Apache Airflow - Set Up</h4>
											
										<pre class="py-code">
											<code style="margin: 0; padding: 0;">
		<span class="code-comment"># Apache Airflow - Local Set-Up</span>
		$ python3 -m venv airflow-env <span class="code-comment"> # create virtual environment</span>
		$ source airflow-env/bin/activate <span class="code-comment"> # activate virtual environment</span>
		$ export AIRFLOW_HOME=~/airflow 
		$ pip install apache-airflow
		$ airflow db init
		$ airflow webserver -p 8080 <span class="code-comment">#launch webserver</span>
		$ airflow sheduler <span class="code-comment">#launch scheduler</span>
												</code>
											</pre>

											<pre class="py-code">
												<code style="margin: 0; padding: 0;">
			<span class="code-comment"># installing dependencies</span>
			pip install google-cloud-bigquery
			pip install --upgrade snowflake-connector-python
												</code>
											</pre>

									<h3 style="color: #949494; text-align:center; margin-bottom: 0;" id="content">Airflow Weberver UI</h3>
									<a href="images/ingestion.png" target="_blank"><img class="image" src="images/Pasted image (2).png"/></a>
									<h4 style="color: #949494; text-align:center; margin-bottom: 0;" id="content">Key Questions</h4>
									<p style="color: #545454; width: 70%; margin: 50px auto; text-align: justify; font-size: 19px;">It's crucial to address several key questions to ensure the success of the initiative. These questions help clarify the scope, requirements, and challenges associated with building and maintaining scalable pipelines. Five key questions: <br>
										<!-- I. Where is our data?  Source <br>
										II. Where do we consolidate our data?  Storage <br>
										III. How will we get it there?  Ingestion <br>
										IV. How will we clean it up?  Transformation <br>
										V. How will we analyze it?  Reporting <br> -->
										
									</p> 

									<pre style="color: #545454; font-size: 16px;">
						I. Where is our data?  <span class="highlight">Source</span>
						II. Where do we consolidate our data? <span class="highlight">Storage</span>
						III. How will we get it there? <span class="highlight">Ingestion</span>
						IV. How will we clean it up? <span class="highlight">Transformation</span>
						V. How will we analyze it? <span class="highlight">Reporting</span>
									</pre>

									<a href="images/IV.png" target="_blank"><img class="image"src="images/IV.png" alt=""/></a>
									<a href="images/XI.png" target="_blank"><img class="image" src="images/XI.png" alt="" /></a>

									
									<h4 style="color: #949494; text-align:center; margin-top: 40px;" id="content">Project's File Structure</h4>
									<p class="paragraph">Implementing the project's file structure ensures separation of concerns and makes it easier to scale your ETL/ELT projects, debug issues, and onboard new team members.</p>

									<p class="paragraph">
										<span class="text-bold">config/</span> - Configuration files, such as environment variables, database connections, and credentials. <br>
										<span class="text-bold">dags/</span> - For Airflow-based projects, this directory contains the DAGs defining your ETL workflow. <br>
										<span class="text-bold">extract/</span> - Contains scripts for extracting data from various sources (databases, APIs, flat files, etc.). <br>
										<spam class="text-bold">transform/</spam> - Transformation logic is applied here, e.g., data cleaning, formatting, normalization, or aggregation. <br>
										<span class="text-bold">load/</span> - Loading scripts to push the transformed data into the final destination (data warehouse, databases). <br>
										<span class="text-bold">utils/</span> - Utility scripts such as common functions for error handling, logging, or reusable snippets. <br>
										<span class="text-bold">logs/</span> - Centralized directory for log files to monitor your ETL runs. You may want to separate logs by date or pipeline. <br>
										<span class="text-bold">tests/</span> - Unit and integration tests to ensure each component works as expected. <br>
										<span class="text-bold">data/</span> - A directory to temporarily store input or output data files (e.g., CSVs, JSONs) before or after processing. <br>
										<span class="text-bold">docs/</span> - Documentation for the ETL process. This could include setup instructions, data flow diagrams, or API documentation. <br>
										</p>


										<!-- CONFIGURATION CLASS-->
										<h5 style="color: #949494; text-align:center; margin-top: 40px;" id="content">config/</h5>

										<pre class="py-code">
											<code>
		<spam class="code-comment"># config/config.yaml</spam>
		source:
		type: database
		database:
			engine: postgresql
			host: localhost
			port: 5432
			name: adw_db
			user: **
			password: ${SOURCE_DB_PASSWORD}  # Referenced

		target:
		type: data warehouse
		database:
			engine: postgresql
			host: localhost
			port: 5432
			name: target_db
			user: target_user
			password: ${TARGET_DB_PASSWORD}  # Referenced
			table: "processed_users"
										</code>
									</pre>

									<h5 style="color: #949494; text-align:center; margin-top: 40px;" id="content">dags/</h5>
									<p class="paragraph" style="text-align: center;">**Return back later once whole scripts is in place.**</p>

									<!-- EXTRACT CLASS -->
									<h5 style="color: #949494; text-align:center; margin-top: 40px;" id="content">extract/<br>implementing extract class</h5>

									<pre class="py-code">
										<code style="margin: 0; padding: 0;">
		<span class="code-comment"># extract class</span>
		<span class="py-method">import</span> pandas as pd
		<span class="py-method">import</span> requests
		<spam class="py-method">import</spam> sqlalchemy

		<span class="code-func">class</span> Extractor:
			<span class="code-func">def</span> __init__(self, source_type, config):
				self.source_type = source_type
				self.config = config

			<span class="code-func">def</span> extract(self):
				if self.source_type == 'csv':
					return self._extract_csv()
				elif self.source_type == 'api':
					return self._extract_api()
				elif self.source_type == 'database':
					return self._extract_database()
				else:
					raise ValueError(f"Unsupported source type: {self.source_type}")

			<span class="code-func">def</span> _extract_csv(self):
				filepath = self.config.get('filepath')
				df = pd.read_csv(filepath)
				print(f"Extracted {len(df)} records from CSV.")
				return df

			<span class="code-func">def</span> _extract_api(self):
				url = self.config.get('url')
				response = requests.get(url)
				data = response.json()
				df = pd.json_normalize(data)
				print(f"Extracted {len(df)} records from API.")
				return df

			<span class="code-func">def</span> _extract_database(self):
				connection_string = self.config.get('connection_string')
				query = self.config.get('query')
				engine = sqlalchemy.create_engine(connection_string)
				df = pd.read_sql_query(query, engine)
				print(f"Extracted {len(df)} records from Database.")
				return df
										</code>
									</pre>

									<!-- TRANSFORM CLASS -->
									<h5 style="color: #949494; text-align:center; margin-top: 40px;" id="content">transform/<br>implementing transform class</h5>

									<pre class="py-code">
										<code style="margin: 0; padding: 0;">
		<span class="code-comment">Transform Class</span>
		<span class="code-func">class</span> Transformer:
		<span class="code-func">def</span> __init__(self, transformation_functions=None):
			<span class="code-text">
			"""
			transformation_functions: List of functions to apply to the dataframe
			"""
			self.transformation_functions = transformation_functions or []
			</span> 

		def transform(self, df):
			for func in self.transformation_functions:
				df = func(df)
			print("Data transformation complete.")
			return df
										</code>
									</pre>

									<!-- LOAD CLASS -->
									<h5 style="color: #949494; text-align:center; margin-top: 40px;" id="content">load/<br>implementing load class</h5>

									<pre class="py-code">
										<code style="margin: 0; padding: 0;">
		<span class="code-comment"># Task Two - extract_load</span>
		class Loader:
		def __init__(self, destination_type, config):
			self.destination_type = destination_type
			self.config = config

		def load(self, df):
			if self.destination_type == 'database':
				return self._load_database(df)
			elif self.destination_type == 'csv':
				return self._load_csv(df)
			else:
				raise ValueError(f"Unsupported destination type: {self.destination_type}")

		def _load_database(self, df):
			connection_string = self.config.get('connection_string')
			table_name = self.config.get('table_name')
			engine = sqlalchemy.create_engine(connection_string)
			df.to_sql(table_name, engine, if_exists='append', index=False)
			print(f"Loaded {len(df)} records into database table '{table_name}'.")
			return

		def _load_csv(self, df):
			filepath = self.config.get('filepath')
			df.to_csv(filepath, index=False)
			print(f"Loaded {len(df)} records into CSV file at '{filepath}'.")
			return

										</code>
									</pre>

									<h4 style="color: #949494; text-align:center; margin-bottom: 0;" id="content">Orchestrating Pipeline - Apache Airflow</h4>
									<a href="images/VIII.png" target="_blank"><img class="image" src="images/VIII.png" alt="" /></a>
									<a href="images/XII.png" target="_blank"><img class="image" src="images/XII.png" alt="" /></a>
									<h4 style="color: #949494; text-align:center; margin-top: 70px; margin-bottom: 0;" id="content">Running dbt Models</h4>
									<a href="images/XIII.png" target="_blank"><img class="image" src="images/XIII.png" alt="" /></a>
								
					</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>